main:
  steps:
    - init:
        assign:
          - uri: "gs://YOUR_PROJECT_ID-csv-bucket/raw/latest.csv"
          - dataset: "data_pipeline"
    - load_to_bq:
        call: googleapis.bigquery.v2.jobs.insert
        args:
          projectId: YOUR_PROJECT_ID
          configuration:
            load:
              sourceUris: [${uri}]
              destinationTable:
                datasetId: ${dataset}
                tableId: "staging"
              sourceFormat: "CSV"
              skipLeadingRows: 1
              writeDisposition: "WRITE_APPEND"
    - clean_transform:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: YOUR_PROJECT_ID
          body:
            query: |
              CREATE OR REPLACE TABLE `${dataset}.cleaned` AS
              SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC) AS rn
              FROM `${dataset}.staging`
              WHERE id IS NOT NULL
            useLegacySql: false
    - merge_final:
        call: googleapis.bigquery.v2.jobs.query
        args:
          projectId: YOUR_PROJECT_ID
          body:
            query: |
              MERGE `${dataset}.final` T
              USING (SELECT * FROM `${dataset}.cleaned` WHERE rn = 1) S
              ON T.id = S.id
              WHEN MATCHED THEN UPDATE SET T.name = S.name
              WHEN NOT MATCHED THEN INSERT(id, name) VALUES(S.id, S.name)
            useLegacySql: false